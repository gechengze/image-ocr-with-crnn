{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4acb8ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "725cba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0123456789.-ABCDEFGHIJKLMNOPQRSTUVWXYZ/\n",
      "{' ': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '.': 11, '-': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, '/': 39}\n"
     ]
    }
   ],
   "source": [
    "with open('./data/alphabet.txt') as f:\n",
    "    alphabet = f.readline()\n",
    "print(alphabet)    \n",
    "\n",
    "# Map the characters in the alphabet to the index\n",
    "alphabet_map = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    # The index of blank in CTCLoss should be zero.\n",
    "    # The first one in the alphabet has been left blank,\n",
    "    # and there is no need for special operation here\n",
    "    alphabet_map[char] = i\n",
    "print(alphabet_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "180877ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process train data\n",
      "image count: 2906 , label count: 2906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2906/2906 [00:00<00:00, 26312.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start process test data\n",
      "image count: 971 , label count: 971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 971/971 [00:00<00:00, 26822.64it/s]\n"
     ]
    }
   ],
   "source": [
    "def pre_process_img_file(label_file, img_dir, new_dir):\n",
    "    \"\"\"Process image file from img_dir to new_dir\n",
    "    \n",
    "    Args:\n",
    "        label_file: The path of label file, each line like \"xxx.jpg 1 2 3 4...\"\n",
    "        img_dir: Origin image files folder.\n",
    "        new_dir: New folder for save images after processed.    \n",
    "    \"\"\"\n",
    "    img_names = []\n",
    "    labels = []\n",
    "    with open(label_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            img_names.append(line.strip('\\n').split(' ')[0].split('/')[1]) \n",
    "            idxs = line.strip('\\n').split(' ')[1:]\n",
    "            labels.append(''.join([alphabet[int(idx)] for idx in idxs]))\n",
    "    print('image count:', len(img_names), ', label count:', len(labels))\n",
    "\n",
    "    if os.path.exists(new_dir):\n",
    "        shutil.rmtree(new_dir)\n",
    "\n",
    "    os.mkdir(new_dir)\n",
    "\n",
    "    for idx, img_name in enumerate(tqdm(img_names)):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        new_path = os.path.join(new_dir, img_name.split('_')[0] + '_' + labels[idx] + '.jpg')\n",
    "        shutil.copyfile(img_path, new_path)\n",
    "           \n",
    "print('start process train data')\n",
    "pre_process_img_file(label_file='./data/data_train.txt',\n",
    "                     img_dir='./data/train_imgs/samples_images/',\n",
    "                     new_dir='./data/train/')\n",
    " \n",
    "print('start process test data')\n",
    "pre_process_img_file(label_file='./data/data_test.txt',\n",
    "                     img_dir='./data/test_imgs/samples_images/',\n",
    "                     new_dir='./data/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5fa162d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=200x32 at 0x7F3DDB407DF0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one of the train image to see if it is right\n",
    "Image.open('./data/train/0_ZOW-PRF-LFB.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1648e0f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAAAgCAAAAAByJGitAAAFOklEQVR4nO3UW4jcVx3A8e+5/e//uWcviSa2CSxaqmSlEKu1NW1BTB5aMfVSiYqKFSrRhjZNWNtCl6hNq1DTiKSCmAdrahrBYCwRRDQPFm0SayvRCIm9bbKbnZmdndm5/efnQ5vqiy+W3af5vP4OnPM9HA4MDQ0NDQ29De6O78F6rjp9+Z7cKLE+WJv0HYF3Ze55gMVGmBjG0LvkAVLC0buW1rEKckRflZ0poAMsVuPwcbsHszNSFbnUWbzw51MzIi8BoVnGEDVd+3XMx0VactIa+6kX5Jh+Z+r/Z4EJiAKFzkPRw3558TgjNi6/IkddZXVR40+1ZOZpVUkhZ0CrCK35qIj0pdtfaGUiIv25E6kGrP8/D/J22acarXCXLIyztXun4qw80loXoopX5jlwYE2eoqECR+c/sx6SM1n7c15qNYWP1aVebakyKVAGUvApfWHHt6dbmcx3u62FhdkXnv00Cg/PLltI7r6+bFm8POmCMV7/WvHizV4m59LR/15SJkAHasepfYpkcCiC0ver5RjwojKN5g/dI7NyqGgLb1x3AD6UIW7Wv0XeB3yDBZ3HLVsHbJTnp2S78tZQWPrSuuzd9Lqylvxb8zENKRToy214UfYo5PnLtM9I5BIwbXmvx4G6/FiTRGiLtigUOl0vrb1ohfbTJCBQuGUM0asHbEhra0339aQTTL8maemvNWq2rt5kZwTV8KndVN33izHT6s+r3MLh8alOcb4VL0I2yCa645f2L40FNFvhwDHQiPJKbvGqvm1YZX3bayy2sZoe2bKFwEOz/T88CdEI9/9TbpfsSan9xmKvhOBUhG/je+VqO0Zlb5ZXaufgpyUqhoLyYn12sX8Dkz/PbiB0lsAajEE79MQv5dwtb25ifNCY5XxZo3tE9j+Xs0rduCCtLd+ZlbneT3zHlRAnTZF6yHyPEGV+u4ciIpt0kSgm1uQHWWPuXxcWt45nLWmn4IHnHKHZ8I/OyWsiz2ijAIMDx/L9Whyt9z8kNxm2XJyT2kRp293bu8+FzhBE4CHS6op0uKbxUjLCatq7WK8a+8PQrSGHLUUPv1qVRl3qGun1Bu1Bu0tZ4QG3nZTDq31njdFq+Y7/FlOT5tr5Z++ZbsmZA7IJrW17fhJFa0lkqbHQldaSzE7cVZfa6aePV2Xnu0BWkWy8dcJ3lt0d+WwxObwoVF6TgYhINqgvQVjm3pdfvSPFWqPVSoQ82rzUgGDDhlWs+WbzTh+4INOp9RsiXRHBolRCEqoPbtt8bf2c4lqy67978fxD+5ofKTzY74WMxw9L1ZLE4BQjbZEq5D58Vv50dQ5jVijkiZ4cgYAR4MjcY2jFE50TKAqgfPAJShDgO+eYlC5O+e3fZV9cw2bZG3xlRggNUx1xkAQFHwIMJQcPiBwbCzF6ZTr4Wb/2Ix9tSVZFB3vHTIl4u5wHQlKLyYFHHJkkVMTmqezlCmV6DcHa04Nb9Q+kltfqA9L6O3gAcYxTCpR7cKY5ZfC1UisT8rhUT+sw1hZ4vPMrBbpwojoeqxwQFKm88dFYvBLImY3gqVpX0uPyjVykveezG285UJfNGopEGoPDMqo9b09fNuFYoQz4xKGOeADmHexd+j0klrvlyGNQcSiMIkw8G5OHqNTbk0vLcN35V9pLtUJIVNrRlP7c5a8T5xT4HoBvQiiw7dSLBYKViQDw1kkHMBb0ff1jjhzRe/54/m+hBoeFCuShSIKzF94HPnnL7mcO3uyRs7hPvijZ+0kUAQocygcvAuz1n/cs4cqVDA0NDQ0N/f/+Df4i9j9z3AxaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=200x32 at 0x7F3DDB3FE370>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check one of the test image to see if it is right\n",
    "Image.open('./data/test/0_199-468-T.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7e3261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"Create dataset inherited from torch.utils.data.Dataset\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir: train dir or test dir.\n",
    "        alphabet_map: The map from char to index.\n",
    "        img_names: File names of all image under the data_dir.\n",
    "        lables: Labels of all image under the data_dir.\n",
    "        trans: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "        to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"Inits dataset\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.alphabet_map = alphabet_map\n",
    "        self.img_names = os.listdir(self.data_dir)\n",
    "        self.labels = [i.split('_')[1].split('.')[0] for i in self.img_names]\n",
    "        self.trans = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get single image by idx\n",
    "        \n",
    "        Args:\n",
    "            idx: index\n",
    "            \n",
    "        Returns:\n",
    "            img: torch.FloatTensor\n",
    "            label: Actual lable of the image, like \"ZOW-PRF-LFB\".\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.data_dir, self.img_names[idx])\n",
    "        img = Image.open(img_path)\n",
    "        img = self.trans(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "class BiLSTM(nn.Module):\n",
    "    \"\"\" Bidirectional LSTM and embedding layer.\n",
    "    \n",
    "    Attributes:\n",
    "        rnn: Bidirectional LSTM\n",
    "        linear: Embedding layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input, num_hiddens, num_output):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(num_input, num_hiddens, bidirectional=True)\n",
    "        # the size of input of embedding layer should mutiply by 2, because of the bidirectional.\n",
    "        self.linear = nn.Linear(num_hiddens * 2, num_output)  \n",
    "    \n",
    "    def forward(self, X):\n",
    "        rnn_out, _ = self.rnn(X)\n",
    "        T, b, h = rnn_out.size()  # T: time step, b: batch size, h: hidden size\n",
    "        t_rec = rnn_out.view(T * b, h)\n",
    "        output = self.linear(t_rec)\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    \"\"\"CRNN net, refer to the paper from https://arxiv.org/pdf/1507.05717v1.pdf.\n",
    "    \n",
    "    Attributes:\n",
    "        cnn: nn.Sequential include conv2d/relu/maxpool2d/batchnorm layers,\n",
    "        input size (1 x 32 X 200), output size ()\n",
    "        rnn: \n",
    "    \"\"\"\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1), dilation=1, ceil_mode=False),\n",
    "            nn.Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1)),\n",
    "            nn.BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.rnn = nn.Sequential(\n",
    "            BiLSTM(512, 256, 256),\n",
    "            BiLSTM(256, 256, num_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        cnn_out = self.cnn(X)  # cnn_out shape: (batch_size x channel x height x width)\n",
    "        assert cnn_out.shape[2] == 1, \"the height of conv must be 1\"\n",
    "        cnn_out = cnn_out.squeeze(2)  # squeeze the dim 2 (height) of cnn_out\n",
    "        cnn_out = cnn_out.permute(2, 0, 1)  # move the width to the first dim, as the time step of rnn input\n",
    "        output = self.rnn(cnn_out)  # output shape: (time step x batch_size x num_class)\n",
    "        output = F.log_softmax(output, dim=2)  # do softmax at the dim of num_class\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4c65b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([64, 1, 32, 200])\n",
      "output shape from CRNNnet: torch.Size([51, 64, 40])\n"
     ]
    }
   ],
   "source": [
    "train_set = MyDataset(data_dir='./data/train/')\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Check if the input and output shapes meet expectations\n",
    "for X, y in trainloader:\n",
    "    break\n",
    "print('input shape:', X.shape)\n",
    "crnn = CRNN(num_class=len(alphabet))\n",
    "preds = crnn(X)\n",
    "print('output shape from CRNNnet:', preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b2e9db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctcloss_parameters(text_batch):\n",
    "    \"\"\"Convert the real text batch into three parameters required by ctcloss,\n",
    "    encoded text/predict length/real length\n",
    "    \n",
    "    Args:\n",
    "        text_batch: real text batch, like('E-Z-4', 'EMD-6-04')\n",
    "        \n",
    "    Returns:\n",
    "        encoded_text: encode text by alphabet_map \n",
    "        preds_length: (time step x batch_size) => (51 * batch_size)\n",
    "        actual_length: length of text to index，max(len(text)) * batch_size\n",
    "    \"\"\"\n",
    "    actual_length = []\n",
    "    result = []\n",
    "    for item in text_batch:            \n",
    "        actual_length.append(len(item))\n",
    "        r = []\n",
    "        for char in item:\n",
    "            index = alphabet_map[char]\n",
    "            r.append(index)\n",
    "        result.append(r)\n",
    "\n",
    "    max_len = 0\n",
    "    for r in result:\n",
    "        if len(r) > max_len:\n",
    "            max_len = len(r)\n",
    "\n",
    "    result_temp = []\n",
    "    for r in result:\n",
    "        for i in range(max_len - len(r)):\n",
    "            r.append(0)\n",
    "        result_temp.append(r)\n",
    "\n",
    "    encoded_text = result_temp\n",
    "    encoded_text = torch.LongTensor(encoded_text)\n",
    "    preds_length = torch.LongTensor([preds.size(0)] * len(y))\n",
    "    actual_length = torch.LongTensor(actual_length)\n",
    "    return encoded_text, preds_length, actual_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44c10f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1          loss: 0.028812\n",
      "epoch 2          loss: 0.026354\n",
      "epoch 3          loss: 0.020040\n",
      "epoch 4          loss: 0.015177\n",
      "epoch 5          loss: 0.009884\n",
      "epoch 6          loss: 0.007474\n",
      "epoch 7          loss: 0.005545\n",
      "epoch 8          loss: 0.004006\n",
      "epoch 9          loss: 0.003555\n",
      "epoch 10         loss: 0.003011\n",
      "epoch 11         loss: 0.002031\n",
      "epoch 12         loss: 0.001427\n",
      "epoch 13         loss: 0.002075\n",
      "epoch 14         loss: 0.001217\n",
      "epoch 15         loss: 0.001311\n",
      "epoch 16         loss: 0.001146\n",
      "epoch 17         loss: 0.000557\n",
      "epoch 18         loss: 0.000527\n",
      "epoch 19         loss: 0.000897\n",
      "epoch 20         loss: 0.000370\n",
      "epoch 21         loss: 0.000171\n",
      "epoch 22         loss: 0.000330\n",
      "epoch 23         loss: 0.000222\n",
      "epoch 24         loss: 0.000159\n",
      "epoch 25         loss: 0.000173\n",
      "epoch 26         loss: 0.000219\n",
      "epoch 27         loss: 0.000212\n",
      "epoch 28         loss: 0.000108\n",
      "epoch 29         loss: 0.000133\n",
      "epoch 30         loss: 0.000333\n",
      "epoch 31         loss: 0.000419\n",
      "epoch 32         loss: 0.000167\n",
      "epoch 33         loss: 0.000955\n",
      "epoch 34         loss: 0.000363\n",
      "epoch 35         loss: 0.000250\n",
      "epoch 36         loss: 0.000540\n",
      "epoch 37         loss: 0.000105\n",
      "epoch 38         loss: 0.000126\n",
      "epoch 39         loss: 0.000576\n",
      "epoch 40         loss: 0.000051\n",
      "epoch 41         loss: 0.000262\n",
      "epoch 42         loss: 0.000276\n",
      "epoch 43         loss: 0.000275\n",
      "epoch 44         loss: 0.000161\n",
      "epoch 45         loss: 0.000110\n",
      "epoch 46         loss: 0.000049\n",
      "epoch 47         loss: 0.000015\n",
      "epoch 48         loss: 0.000011\n",
      "epoch 49         loss: 0.000006\n",
      "epoch 50         loss: 0.000006\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "num_epoch = 50\n",
    "\n",
    "if use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "crnn.train()\n",
    "trainer = torch.optim.Adam(crnn.parameters(), lr=0.001)\n",
    "loss = nn.CTCLoss(zero_infinity=True)\n",
    "crnn = crnn.to(device)\n",
    "loss = loss.to(device)\n",
    "    \n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in trainloader:\n",
    "        X = X.to(device)\n",
    "        trainer.zero_grad()\n",
    "        preds = crnn(X) \n",
    "        encoded_text, preds_length, actual_length = get_ctcloss_parameters(y)\n",
    "\n",
    "        encoded_text = encoded_text.to(device)\n",
    "        preds_length = preds_length.to(device)\n",
    "        actual_length = actual_length.to(device)\n",
    "        \n",
    "        l = loss(preds, encoded_text,preds_length, actual_length) / batch_size\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    print('epoch', str(epoch + 1).ljust(10), 'loss:', format(l.item(), '.6f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0672617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_pred(text):\n",
    "    \"\"\"Remove adjacent duplicate characters\n",
    "\n",
    "    Args:\n",
    "        text: Do argmax after crnn net ouput\n",
    "        \n",
    "    Returns:\n",
    "        final_text: Text removed adjacent duplicate characters\n",
    "    \"\"\"\n",
    "    text = list(text)\n",
    "    for i in range(len(text)):\n",
    "        for j in range(i + 1, len(text)):\n",
    "            if text[j] == ' ':\n",
    "                break\n",
    "            else:\n",
    "                if text[j] == text[i]:\n",
    "                    text[j] = ' '\n",
    "                else:\n",
    "                    continue\n",
    "    final_text = ''.join(text).replace(' ', '')\n",
    "    return final_text\n",
    "\n",
    "def predict(net, X, y):\n",
    "    \"\"\"Predict batch images, print predict result and ground truth.\n",
    "    \n",
    "    Args:\n",
    "        net: crnn net\n",
    "        X: batch images\n",
    "        y: batch actual texts\n",
    "    \"\"\"\n",
    "    preds = net(X)\n",
    "    _, preds = preds.max(2)\n",
    "    idx = 0\n",
    "    print('crnn net output'.ljust(51), '|', 'final predict'.ljust(20), '|', 'ground truth'.ljust(20))\n",
    "    print('=' * 99)\n",
    "    for pred in preds.permute(1, 0):\n",
    "        pred_text = ''.join([alphabet[i.item()] for i in pred])\n",
    "        print(pred_text, '|', get_final_pred(pred_text).ljust(20), '|', y[idx].ljust(20))\n",
    "        print('·' * 99)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af514a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crnn net output                                     | final predict        | ground truth        \n",
      "===================================================================================================\n",
      "V    --6   --R   E   K                              | V-6-REK              | V-6-REK             \n",
      "···································································································\n",
      "crnn net output                                     | final predict        | ground truth        \n",
      "===================================================================================================\n",
      "F   N    W   TT     --F   P  P     --S   W          | FNWT-FPP-SW          | FNWT-FPP-SW         \n",
      "···································································································\n",
      "U  I A   --C   S   S   A      --R   O   M   I       | UIA-CSSA-ROMI        | UR-CSSK-ROAI        \n",
      "···································································································\n",
      "6   3   --B   4     --U   C   A                     | 63-B4-UCA            | 63-BH-UCA           \n",
      "···································································································\n",
      "1 1  3      --Y  P      --V                         | 113-YP-V             | 113-YP-V            \n",
      "···································································································\n",
      "4   6   --W       --F   K    H                      | 46-W-FKH             | 46-W-FKH            \n",
      "···································································································\n",
      "D    A    --U      --6  5  33                       | DA-U-653             | DA-U-663            \n",
      "···································································································\n",
      "V   P  Z    S     --E   V    U    --B   I C         | VPZS-EVU-BIC         | VPZS-EVO-AIC        \n",
      "···································································································\n",
      "G   P     --X    --4                                | GP-X-4               | GP-X-4              \n",
      "···································································································\n"
     ]
    }
   ],
   "source": [
    "test_set = MyDataset(data_dir='./data/test/')\n",
    "\n",
    "# predict single image with random index\n",
    "idx = random.randint(0, len(test_set) - 1)\n",
    "X, y = test_set[idx]\n",
    "X = X.unsqueeze(0) # add dim as batch\n",
    "y = [y]\n",
    "X = X.to(device)\n",
    "predict(crnn, X, y)\n",
    "\n",
    "# predict batch using dataloader\n",
    "testloader = DataLoader(test_set, batch_size=8, shuffle=True, drop_last=True)\n",
    "X, y = next(iter(testloader))\n",
    "X = X.to(device)\n",
    "predict(crnn, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a8502e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
